from pyspark import SparkContext, SparkConf
import sys
import random
import math
import utils
import types

from utils import Instance

from matrix import Matrix

s = []
y = []
p = []

M = 10

def get_Hk_gk(k,grad_vec):
    global s
    global y
    global p
    global M

    if k <= M:
        L = k
    else:
        L = M

    q = grad_vec

    alpha = [0] * L

    for i in range(L-1,-1,-1):

        res = s[i].Trans() * q * p[i]
        
        if res.row != 1 or res.col != 1:
            raise TypeError("dimension error for res")

        alpha[i] = res[0][0]

        q = q - y[i] * alpha[i]

    beta = [0]*L

    z = q
    '''
    tmp=y[L-1].Trans()
    Hk_fenzi = tmp*s[L-1]
    Hk_fenmu = tmp*y[L-1]

    Hk = Hk_fenzi[0][0] / Hk_fenmu[0][0]

    z = q * Hk
    '''

    for i in range(0,L):
        
        res = y[i].Trans() * z * p[i]

        if res.row != 1 or res.col != 1:
            raise TypeError("dimension error for res")

        beta[i] = res[0][0]

        z = z + s[i]*(alpha[i]-beta[i])

    return z

def normalization(ins_grad,ins_count,weights,feat_dict,theta):

    grad = {}

    for f in ins_grad:
        g = f[1]/ins_count
        if f[0] in weights:
            g += theta*weights[f[0]]

        grad[f[0]] = g

    return Matrix(value=grad,idx_dict=feat_dict)

def calc_gradient(ins,weights):
        
    grad = []

    if True:#random.randint(1,int(1/sampling_rate)) == 1:

        pred = ins.predict(weights.value)

        for f in ins.feat:
            grad.append((f,(pred - ins.label)))
        
    return grad

def eval_ins_map(ins,weights):
    return ( ins.predict(weights.value), ins.label)

def update_weight(step,feat_dict,feat_weight):

    for f in feat_dict:
        v = step[feat_dict[f]][0]

        if v != 0:
            if f not in feat_weight:
                feat_weight[f] = 0.0

            feat_weight[f] -= v

def ListToDict(_list_):
    _dict_ = {}

    for i in _list_:
        _dict_[i[0]] = i[1]

    return _dict_

def train(sc):

    global s
    global y

    feat_weight = {}

    learning_rate = 0.5
    ITER_MAX = 1000
    THETA = 4
    SAMPLING_RATE = 0.01

    [train_ins,train_ins_count] = utils.load_ins(sc,"hdfs://hqz-ubuntu-master:9000/data/filtered_ins/train/part-00051")
    
    [eval_ins,eval_ins_count] = utils.load_ins(sc,"hdfs://hqz-ubuntu-master:9000/data/filtered_ins/eval/*")
    
    feat_dict = utils.load_feat(sc,"hdfs://hqz-ubuntu-master:9000/data/filtered_feat/*")
 
    cur_iter = 0
    
    broadcast_feat = sc.broadcast(feat_weight)

    ins_grad = train_ins.flatMap(lambda ins: calc_gradient(ins, broadcast_feat)).reduceByKey(lambda a,b: a+b).collect()

    grad_vector = normalization(ins_grad,train_ins_count,feat_weight,feat_dict,THETA)
    
    d = grad_vector

    while cur_iter < ITER_MAX:
        
        print ( "iteration %d" % cur_iter )

        si = d * ( learning_rate / math.sqrt(cur_iter + 1) )

        #print "si"
        #print type(si)

        if len(s) >= M:
            s.pop(0)

        s.append(si)

        update_weight(si,feat_dict,feat_weight)

        broadcast_feat = sc.broadcast(feat_weight)
        
        eval_res = train_ins.map(lambda ins: eval_ins_map(ins,broadcast_feat)).sortByKey().collect()

        [auc, mae] = utils.get_eval_stat(eval_res)
        
        print ("selected %d samples: train_set : auc :%f, mae: %f" % (train_ins_count,auc,mae))

        eval_res = eval_ins.map(lambda ins: eval_ins_map(ins,broadcast_feat)).sortByKey().collect()

        [auc, mae] = utils.get_eval_stat(eval_res)
        
        utils.output(cur_iter, None, feat_weight,eval_res)
        
        print ("test_set: auc :%f, mae: %f" % (auc,mae))

        ins_grad = train_ins.flatMap(lambda ins: calc_gradient(ins, broadcast_feat)).reduceByKey(lambda a,b: a+b).collect()

        new_grad_vector = normalization(ins_grad,train_ins_count,feat_weight,feat_dict,THETA)
        #Matrix(value=ListToDict(new_grad),idx_dict=feat_dict)
        
        yi = new_grad_vector - grad_vector

        if len(y) >= M:
            y.pop(0)
        
        y.append(yi)

        if len(p) >= M:
            p.pop(0)

        tmp = yi.Trans()*si

        if tmp.row != 1 or tmp.col != 1:
            raise TypeError("dimension error for tmp")

        p.append(1/(tmp[0][0]))

        grad_vector = new_grad_vector

        cur_iter += 1

        d =  get_Hk_gk(cur_iter,grad_vector)
        
        #print "d:"
        #print type(d)

if __name__ == "__main__":

    conf = SparkConf().setAppName("LR_SGD")
    sc = SparkContext(conf=conf)

    train(sc)
